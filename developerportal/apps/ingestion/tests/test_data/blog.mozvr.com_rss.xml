<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Mozilla Mixed Reality Blog]]></title><description><![CDATA[We are the Mozilla MR team. Our goal is to help bring high-performance mixed reality to the open Web.]]></description><link>https://blog.mozvr.com/</link><image><url>https://blog.mozvr.com/favicon.png</url><title>Mozilla Mixed Reality Blog</title><link>https://blog.mozvr.com/</link></image><generator>Ghost 2.37</generator><lastBuildDate>Thu, 02 Jan 2020 18:45:32 GMT</lastBuildDate><atom:link href="https://blog.mozvr.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Happy New Year from Hubs!]]></title><description><![CDATA[<p>As we wrap up 2019, The Hubs team says thank you to the Mozilla Mixed Reality Community for an incredible year! We’ve been looking back and we’re excited about the key milestones that we’ve hit in our mission to make private social VR readily available to the</p>]]></description><link>https://blog.mozvr.com/happy-new-year-from-hubs/</link><guid isPermaLink="false">5e0a35549430b30038d5c045</guid><category><![CDATA[social]]></category><dc:creator><![CDATA[Liv Erickson]]></dc:creator><pubDate>Tue, 31 Dec 2019 14:00:00 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/12/Hubs_Year_Review_Social_Post.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/12/Hubs_Year_Review_Social_Post.png" alt="Happy New Year from Hubs!"><p>As we wrap up 2019, The Hubs team says thank you to the Mozilla Mixed Reality Community for an incredible year! We’ve been looking back and we’re excited about the key milestones that we’ve hit in our mission to make private social VR readily available to the general public. At the core of what we’re doing, our team is exploring the ways that spatial computing and shared environments can improve the ways that we connect and collaborate, and thanks to the feedback and participation of our users and community as a whole, we got to spend a lot of time this year working on new features and experiments.</p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe src="https://player.vimeo.com/video/382051967?app_id=122963" width="426" height="240" frameborder="0" allow="autoplay; fullscreen" allowfullscreen title="Mozilla Hubs 2019 in Review"></iframe></figure><!--kg-card-end: embed--><p>Early in the year, we wanted to dive into our hypothesis that social 3D spaces could integrate into our existing platforms and tools that the team was regularly using. We launched the <a href="https://blog.mozvr.com/hubs-discord-beta/">Hubs Discord Bot</a> back in April, which bridged chat between the two platforms and added an optional authentication layer to restrict access to rooms created with the bot to users in a given server. Since launching the Discord bot, we’ve learned more about<a href="https://blog.mozvr.com/creating-privacy-centric-virtual-spaces/"> the behaviors and frameworks that enable healthy community development</a> and management, and we released a <a href="https://blog.mozvr.com/hubs-summer-update-2019/">series of new features that supported multiple moderators, configurable room permissions</a>, closing rooms, and more.</p><p>One of our goals for this year was to empower users to more easily personalize their Hubs experiences by making it easy to create custom content. This work kicked off with making <a href="https://blog.mozvr.com/spoke-3d-scenes-web/">Spoke</a> available as a hosted web application, so creators no longer had to download a separate application to build scenes for Hubs. We followed with new features that improved<a href="https://blog.mozvr.com/new-avatar-features-in-hubs/"> how avatars</a> could be created, shared, remixed, and discovered, and we wrapped up the year by releasing several pre-configured asset kits for building unique environments, starting with the <a href="https://blog.mozvr.com/year-with-spoke-architecture-kit/">Spoke Architecture Kit release that also included a number of ease-of-use feature</a> updates.</p><p>We’ve also just had a lot of fun connecting with users and growing our team and community, and we’ve learned a lot about what we’re working on and how to improve Hubs for different use cases. When <a href="http://twitter.com/byhubs">we joined Twitter</a>, we got to start interacting with a lot more of you on a regular basis and we’ve loved seeing how you’ve been using Hubs when you share your own content with us! The number of new scenes, avatars, and even public events that have been shared within our community gets us even more excited for what we think 2020 can bring.</p><p>As we look ahead into the next year, we’ll be sharing a big update in January and go in-depth with work we’ve been doing to make Hubs a more versatile platform. If you want to follow along with our roadmap, you can keep an eye on <a href="https://github.com/mozilla/hubs/milestone/4">the work we have planned on GitHub</a> and follow us on <a href="https://twitter.com/byhubs/">Twitter @ByHubs</a>. Happy 2020!</p>]]></content:encoded></item><item><title><![CDATA[How much is that new VR headset really sharing about you?]]></title><description><![CDATA[VR was big this holiday season - the Oculus Go sales hit the Amazon #1 electronics device list on Black Friday. But in the spirit of Mozilla's Privacy Not Included guidelines, you might be wondering: what personal information is Oculus collecting while you use your device?]]></description><link>https://blog.mozvr.com/vr-headset-data-collection/</link><guid isPermaLink="false">5de725f22375ec0038565b6e</guid><category><![CDATA[privacy]]></category><category><![CDATA[oculus]]></category><dc:creator><![CDATA[Diane Hosfelt]]></dc:creator><pubDate>Fri, 20 Dec 2019 14:06:39 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1493497029755-f49c8e9a8bbe?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1493497029755-f49c8e9a8bbe?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="How much is that new VR headset really sharing about you?"><p>VR was big this holiday season - the Oculus Go sales hit the Amazon #1 electronics device list on Black Friday, and the Oculus Quest continues to sell. But in the spirit of Mozilla's <a href="https://foundation.mozilla.org/en/privacynotincluded/">Privacy Not Included</a> guidelines, you might be wondering: what personal information is Oculus collecting while you use your device?</p><p>Reading the Oculus <a href="https://www.oculus.com/legal/privacy-policy/" rel="noreferrer nofollow noopener">privacy policy</a>, they say that they process and collect information like</p><ul><li>information about your environment, physical movements, and dimensions</li><li>location-related information</li><li>information about people, games, content, apps, features, and experiences you interact with</li><li>identifiers that may be unique to you</li><li>and much much more!</li></ul><p>That’s…a lot of data. Most of this data, like processing information about your physical movements is required for basic functionality of most MR experiences. For example, to track whether you avoid an obstacle in BeatSaber, your device needs to know the position of your head in space.</p><p>There’s a difference between <em>processing</em> and <em>collecting</em>. Like we mentioned, you can’t do much without processing certain data. Processing can either happen on the device itself, or on remote servers. Collecting data implies that it is stored remotely for a time period beyond what’s necessary for simply processing it.</p><p>Mozilla’s brand promise to our users is focused on security and privacy. So, while testing the Oculus Quest for Mozilla Mixed Reality products, we needed to know what kind of data was being sent to and from the device during a browsing session. The device has a developer mode that allows you to access advanced features by connecting it to your computer and using <a href="https://developer.android.com/studio/command-line/adb">Android Debug Bridge</a> (adb). From there, we used the developer mode and `adb` to install a custom trusted root certificate. This allows us to inspect the connections in depth.</p><p>So, what <em>is</em> Facebook transmitting from your device back to Facebook servers during a routine browsing session? From the data we saw, they’re reporting configuration and telemetry data, such as information about how long it took to fetch resources. For example, here’s a graph of the amount of data sent over time from the Oculus VR headset back to Facebook.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.mozvr.com/content/images/2019/12/Screenshot-2019-12-03-14.06.51-1.png" class="kg-image" alt="How much is that new VR headset really sharing about you?"><figcaption>Bytes sent to Facebook IPs over time</figcaption></figure><!--kg-card-end: image--><p>The data is identified by both an <code>id</code>, which is consistent across browsing sessions, and a <code>session_id</code>. The <code>id</code> appears to be linked to the device hardware, because linking a Facebook account didn’t change the identifier (or any other information as far as we detected).</p><p>In addition to general timing information, Facebook also receives reports on more granular, URL level timing information that uses a unique URL ID.</p><!--kg-card-begin: code--><pre><code>"time_to_fetch": "1",
"url_uid": "d8657582",
"firstbyte_time": "0",</code></pre><!--kg-card-end: code--><p>Like computers, mixed reality (MR) devices can collect data on the sites you visit and applications you interact with. They also have the ability to collect and transmit large amounts of other data, including biometrically-derived data (BDD). BDD includes any information that may be inferred from biometrics, like gaze, gait, and other nonverbal communication methods. <a href="https://packet39.com/blog/2018/02/25/3dof-6dof-roomscale-vr-360-video-and-everything-in-between/">6DOF</a> devices like the Oculus Quest track both head and body movement. Other devices, like the MagicLeap One and HoloLens 2, also track gaze. This type of data can reveal intrinsic characteristics about users, such as their height. Information about where they look can reveal details about a user’s<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4273641/"> sexual preferences</a> and<a href="https://vhil.stanford.edu/mm/2018/08/bailenson-jamap-protecting-nonverbal.pdf"> powerful insights into their psychology</a>. Innocuous data like facial movements during a task have been used in research to predict <a href="https://vhil.stanford.edu/pubs/2011/automatically-analyzing-facial-feature-movements-to-identify-human-errors/">high or low performers</a>.</p><p>Fortunately, even though its privacy policy would allow it to, today Facebook <em>does not</em> appear to collect any of this MR-specific information from your Oculus VR headset. Instead, it focuses on collecting data about timing, application version, and other configuration and telemetry data. That doesn’t mean that they can’t do so in the future, according to their privacy policy.</p><p>In fact, Facebook <a href="https://uploadvr.com/facebook-ads-vr/">just announced</a> that Oculus VR data will now be used for ads if users are logged into Facebook. Horizon, Facebook's social VR experience, requires a linked Facebook account.</p><p>In addition to the difference between processing and collecting explained above, there’s a difference between committing to not collecting and simply not collecting. It’s not enough for Facebook to just not collect sensitive data now. They should commit not to collect it in the future. Otherwise, they could change the data they collect at any time without informing users of the change. Until BDD is protected and regulated, we need to be constantly vigilant.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://media0.giphy.com/media/U6Qzze3Mt90Ig/giphy.gif" class="kg-image" alt="How much is that new VR headset really sharing about you?"></figure><!--kg-card-end: image--><p>Currently, BDD (and other data that MR devices can track) lacks protections beyond whatever is stipulated in the privacy policy (which is regulated by contract law), so companies often reserve the right to collect and disseminate all the information they might possibly want to, knowing that consumers rarely read (let alone comprehend) the legalese they agree to. It’s time for regulators and legislators to take action and protect <a href="https://www.wired.com/story/google-is-slurping-up-health-dataand-it-looks-totally-legal/">sensitive health</a>, biometric, and derived data from <a href="https://blog.mozvr.com/making-ethical-decisions/">misuse by tech companies</a>.</p>]]></content:encoded></item><item><title><![CDATA[Browsing from the Edge]]></title><description><![CDATA[<p>We are currently seeing two changes in computing: improvements in network bandwidth and latency brought on by the deployment of 5G networks, and a large number of low-power mobile devices and headsets. This provides an opportunity for rich web experiences, driven by off-device computing and rendering, delivered over a network</p>]]></description><link>https://blog.mozvr.com/browsing-from-the-edge/</link><guid isPermaLink="false">5dfa24dbaf1a1e004405742f</guid><dc:creator><![CDATA[Alan Jeffrey]]></dc:creator><pubDate>Thu, 19 Dec 2019 15:52:57 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/12/Screen-Shot-2019-12-09-at-12.07.12-PM.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/12/Screen-Shot-2019-12-09-at-12.07.12-PM.png" alt="Browsing from the Edge"><p>We are currently seeing two changes in computing: improvements in network bandwidth and latency brought on by the deployment of 5G networks, and a large number of low-power mobile devices and headsets. This provides an opportunity for rich web experiences, driven by off-device computing and rendering, delivered over a network to a lightweight user agent.</p><p>As we’ve improved our <a href="https://mixedreality.mozilla.org/firefox-reality/">Firefox Reality browser</a> for VR headsets and the content available on the web kept getting better, we have learned that the biggest things limiting more usage are the battery life and compute capabilities of head-worn devices. These are designed to be as lightweight, cool, and comfortable as possible - which is directly at odds with hours of heavy content consumption. Whether it’s for VR headsets or AR headsets, offloading the computation to a separate high-end machine that renders and encodes the content suitable for viewing on a mobile device or headset can enable potentially massive scenes to be rendered and streamed even to low-end devices.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh6.googleusercontent.com/Cx2GVDWo5HY7Ejroc_UN0DN0FcK5jZFKW6NeTJlKT_-QXkNybK-Erb5LJSOnZcf_YWGBhD0EGRX7aqXfJK3-tQ6S-r2lsksRBgBcN7m8hU_YmsnqqU8MQNBGjGi4YN2_jKI_ECTE" class="kg-image" alt="Browsing from the Edge"></figure><!--kg-card-end: image--><p>Mozilla’s Mixed Reality team has been working on embedding Servo, a modern web engine which can take advantage of modern CPU and GPU architectures, into GStreamer, a streaming media platform capable of producing video in a variety of formats using hardware-accelerated encoding pipelines. We have a proof-of-concept implementation that uses Servo as a back end, rendering web content to a GStreamer pipeline, from which it can be encoded and streamed across a network. The plugin is designed to make use of GPUs for hardware-accelerated graphics and video encoding, and will avoid unnecessary readback from the GPU to the CPU which can otherwise lead to high power consumption, low frame rates, and additional latency. Together with <a href="https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/">Mozilla’s Webrender</a>, this means web content will be rendered from CSS through to streaming video without ever leaving the GPU.</p><!--kg-card-begin: html--><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/W9mZb-u3MU0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><!--kg-card-end: html--><p>Today, the GStreamer Servo plugin is available from <a href="https://github.com/servo/servo/tree/master/ports/gstplugin">our Github repo</a>, and can be used to stream 2D non-interactive video content across a network. This is still a work in progress! We are hoping to add immersive, interactive experiences, which will make it possible to view richer content on a wide set of mobile devices and headsets. Contact <a href="mailto:mr@mozilla.com">mr@mozilla.com</a> if you’re looking for specific support for your hardware or platform!</p>]]></content:encoded></item><item><title><![CDATA[Discover on desktop or mobile. Enjoy in VR, only with Firefox Reality.]]></title><description><![CDATA[<p>A special update for Firefox Reality is available today -- just in time for the holidays! Now you can send tabs from your phone or computer straight to your VR headset.</p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/9Ih1mY9rT-U?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><!--kg-card-end: embed--><p>Say you’re waiting in line for your festive peppermint mocha, killing time on your phone. You stumble on</p>]]></description><link>https://blog.mozvr.com/fxr7/</link><guid isPermaLink="false">5de94b7c2375ec0038565bfb</guid><dc:creator><![CDATA[Janice Von Itter]]></dc:creator><pubDate>Thu, 12 Dec 2019 14:00:00 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/12/O_Hero.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/12/O_Hero.png" alt="Discover on desktop or mobile. Enjoy in VR, only with Firefox Reality."><p>A special update for Firefox Reality is available today -- just in time for the holidays! Now you can send tabs from your phone or computer straight to your VR headset.</p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/9Ih1mY9rT-U?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><!--kg-card-end: embed--><p>Say you’re waiting in line for your festive peppermint mocha, killing time on your phone. You stumble on an epic 3D roller coaster video that would be great to watch in VR. Since you’ve already signed in to your <a href="https://www.mozilla.org/en-US/firefox/accounts/">Firefox Account</a> on Firefox Reality, you can send that video right to your headset, where it will be ready to watch next time you open the app. You can also send tabs from VR over to your phone or desktop, for when you eventually take your headset off.</p><p>When you use Firefox on multiple devices, you can sync your history and bookmarks too. No more waving the laser pointer around to type wonky URLs or trying retrace your steps back to that super funny site from yesterday. Stay tuned in the new year for more features like these that make using VR a more seamless part of your everyday life.</p><h3 id="but-wait-there-s-more">But wait, there's more</h3><p>We’ve also added a few small-but-mighty features that our users have requested. First is the ability to copy and paste text and links. Similar to how you do it on your phone, you can press-and-hold to get a menu of choices. With this update you can also use your Bluetooth keyboard to type, if your device has Bluetooth capabilities.</p><p>And finally, we’ve added six new languages and custom keyboards for Swedish, Finnish, Norwegian, Danish, Dutch and Polish.</p><p>Full release notes for this and all our updates can be found in our <a href="https://github.com/MozillaReality/FirefoxReality/wiki/Release-notes-for-version-7">GitHub repository</a>.</p><p>Firefox Reality 7 is available right now.<br><a href="https://www.oculus.com/experiences/go/2208418715853974/"><strong>Download for Oculus Go</strong></a><br><a href="https://www.oculus.com/experiences/quest/2180252408763702"><strong>Download for Oculus Quest</strong></a><br><a href="https://www.viveport.com/mobileapps/05634fed-6dc5-4aa8-865d-af6027f4ec09"><strong>Download for Viveport</strong></a></p><p><em>PS: We want to hear from you! Starting now, we’ll be publishing a survey with each update. You can find the link inside Firefox Reality on the Settings page or just click <a href="https://qsurvey.mozilla.com/s3/FxR">here</a>.</em></p>]]></content:encoded></item><item><title><![CDATA[Getting WebXR to 1.0]]></title><description><![CDATA[<p>As the WebXR standard goes through the final stretch to hit 1.0, we have updated our tools to the final API. <a href="https://www.w3.org/TR/webxr/">WebXR</a> is the new standard for virtual and augmented reality on the web. It lets web developers create immersive experiences without native code or installing an app. People</p>]]></description><link>https://blog.mozvr.com/webxr-1-0-is-here/</link><guid isPermaLink="false">5deebdcb2375ec0038565c20</guid><dc:creator><![CDATA[Josh Marinacci]]></dc:creator><pubDate>Tue, 10 Dec 2019 22:50:02 GMT</pubDate><content:encoded><![CDATA[<p>As the WebXR standard goes through the final stretch to hit 1.0, we have updated our tools to the final API. <a href="https://www.w3.org/TR/webxr/">WebXR</a> is the new standard for virtual and augmented reality on the web. It lets web developers create immersive experiences without native code or installing an app. People can browse VR catalogs, play VR games, and view 360 videos.  On the AR side, you can build a web app that places objects in real 3D space inside of a viewer’s living room, while still protecting user privacy and security. It is still in the draft state, but we don’t expect any more API changes before it hits Candidate Release (CR) in early 2020.</p><h2 id="tool-updates">Tool Updates</h2><p>We’ve put together a collection of tools to help you get started. Many of your favorite web graphics libraries like <a href="https://threejs.org/docs/#manual/en/introduction/How-to-create-VR-content">ThreeJS</a> and <a href="https://doc.babylonjs.com/how_to/webxr">Babylon</a> support WebXR now.  The newly <a href="https://github.com/immersive-web/webxr-polyfill">WebXR polyfill</a> will bridge compatibility between older browsers and newer ones.  We’ve also updated our tools to help you out.</p><h3 id="webxr-emulator">WebXR Emulator</h3><p>We just updated the <a href="https://blog.mozvr.com/webxr-emulator-extension/">WebXR Emulator</a> add-on for Firefox and Chrome with the 1.0 API and a brand new look and feel</p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/uneEvM40dN8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><!--kg-card-end: embed--><h3 id="ecsy-dev-tool">ECSY Dev Tool</h3><p><a href="https://ecsy.io/">ECSY</a>, an entity component system for the Web, now has its own <a href="https://blog.mozvr.com/ecsy-developer-tools/">dev tool add-on</a>, with remote debugging to test on real headsets. Oh, and it looks gorgeous! </p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe width="459" height="344" src="https://www.youtube.com/embed/17UXZ_WXYoQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><!--kg-card-end: embed--><h3 id="immersive-web-components">Immersive Web Components</h3><p><a href="https://blog.mozvr.com/custom-elements-for-the-immersive-web/">Immersive Web Components</a> is now updated to the 1.0 API. This project lets you drop a 360 image or video into your page without writing any Javascript at all.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://blog.mozvr.com/content/images/2019/12/img360-1.png" class="kg-image"></figure><!--kg-card-end: image--><ul><li>We are also in the process of updating our iOS XR Viewer as well.</li></ul><p>You can learn about these tools and more at our new <a href="https://developer.mozilla.com/topics/mixed-reality/">WebXR Developer Portal</a>. Also check out <a href="https://immersiveweb.dev/">immersiveweb.dev</a> with more great WebXR info.</p><h2 id="device-support">Device Support</h2><p>Many browsers support the WebXR API today or will in the next couple of months. This includes both tethered VR headsets attached to your PC running a modern browser, and stand alone headsets running Oculus Browser or Firefox Reality. Browsers that support the older WebVR API will update soon, so you should use the current <a href="https://github.com/immersive-web/webxr-polyfill">WebXR polyfill</a> to bridge compatibility today.‌‌</p><h2 id="next-steps">Next Steps</h2><p>‌‌We expect the WebXR API to add new features (especially AR ones) at a fairly rapid clip. We also expect browser support to continually improve over the next few months. Keep checking back at the Mozilla WebXR <a href="https://developer.mozilla.com/topics/mixed-reality/">Developer Portal</a> to see the latest updates.</p><p>‌‌‌‌‌‌</p><p>‌‌</p>]]></content:encoded></item><item><title><![CDATA[ECSY Developer tools extension]]></title><description><![CDATA[<h1 id="introduction">Introduction</h1><p>Two months ago we <a href="https://blog.mozvr.com/introducing-ecsy/">released ECSY</a>, a framework-agnostic Entity Component System library you could use to build real time applications with the engine of your choice.</p><p>Today we are happy to announce a <strong>developer tools extension</strong> for ECSY, aiming to help you better understand what it is going on</p>]]></description><link>https://blog.mozvr.com/ecsy-developer-tools/</link><guid isPermaLink="false">5deef1a92375ec0038565c42</guid><category><![CDATA[ECSY]]></category><category><![CDATA[developer tools]]></category><category><![CDATA[WebExtension]]></category><dc:creator><![CDATA[Fernando Serrano]]></dc:creator><pubDate>Tue, 10 Dec 2019 22:47:43 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/12/ecsy-header.png" medium="image"/><content:encoded><![CDATA[<h1 id="introduction">Introduction</h1><img src="https://blog.mozvr.com/content/images/2019/12/ecsy-header.png" alt="ECSY Developer tools extension"><p>Two months ago we <a href="https://blog.mozvr.com/introducing-ecsy/">released ECSY</a>, a framework-agnostic Entity Component System library you could use to build real time applications with the engine of your choice.</p><p>Today we are happy to announce a <strong>developer tools extension</strong> for ECSY, aiming to help you better understand what it is going on in your application when using ECSY.</p><p>A common requirement when building applications that require high performance- such as real time 3D graphics, AR and VR experiences- is the need to understand which part of our application is consuming more resources. We could always use the browsers’ profilers to try to understand our bottlenecks but they can be a bit unintuitive to use, and it is hard to get an overview of what is going on in the entire application, rather than focusing on a specific piece of your code.</p><p>With the ECSY developer tools extension you can now see that overview, getting statistics in realtime on how your systems are performing. Once you know which system is causing troubles in performance you can then use the browser profiler to dig into the underlying issue.</p><p>Apart from showing the status of the application, the extension lets you control the execution (eg: play/pause, step system, step frame) for debugging or learning purposes, as well as accessing the value of the queries and components used in your application.</p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe width="459" height="344" src="https://www.youtube.com/embed/17UXZ_WXYoQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><!--kg-card-end: embed--><h1 id="how-do-i-use-it">How do I use it?</h1><ol><li>Install the extension from the extension store for your browser (<a href="https://addons.mozilla.org/en-US/firefox/addon/ecsy-devtools">Firefox</a>, <a href="https://chrome.google.com/webstore/detail/cdmidpfffmlibnnbhkfbobpghgfmhdhk/publish-review?hl=en">Chrome</a>)</li><li>Launch any ECSY application (For example <a href="https://ecsy.io/examples/circles-boxes/index.html">https://ecsy.io/examples/circles-boxes/index.html</a>). You will notice that the “ECSY” icon on the toolbar changes its color indicating that the ECSY library is detected:</li></ol><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh6.googleusercontent.com/FIm-R4UORSoCK1MJctaYskQCwR7HRuueBi_E5VEG3r3auyYdahX60VhFVllcgTFCikyV3i3nnDTtf6DQmfAho8Q2Qa6Sjh1SbFuy361DNASTVHwJFJ67gmjdv7R4PMyRPSwZMg-M" class="kg-image" alt="ECSY Developer tools extension"></figure><!--kg-card-end: image--><ol><li>Open the “ECSY” tab in the browser’s developer tool (<a href="https://developer.mozilla.org/docs/Tools">Firefox</a>, <a href="https://developers.google.com/web/tools/chrome-devtools/">Chrome</a>) and you will see the state of the application.</li></ol><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh4.googleusercontent.com/4-N76zh4B73S-hhR_oM8bnahbUQK3mwJalOcdv0IZuFp4bGAhwpzpF2UYkgRPQsHPvPovN6FwyudAnhNDEabRYmA3buKFzAdeoqG7Ce9zLhwZUGJIwREpF9LDLIq5l9DEeoWnXQz" class="kg-image" alt="ECSY Developer tools extension"></figure><!--kg-card-end: image--><h1 id="panel-overview">Panel overview</h1><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh6.googleusercontent.com/KV62xTgLz3--pCSgz_GB6DLKzuqcxBr7q3q1ISAmG-imOKQpg3cW-CDnUHE3CEKxCcviPXUnbmNZtUQcIlEjkVGyQXsdheI9m9JpD_2y4Q_rZUsNn7OxwTkUaZwzP4XI_mbyh7GW" class="kg-image" alt="ECSY Developer tools extension"></figure><!--kg-card-end: image--><p>When the ECSY extension panel is loaded, you will see all the sections available on the UI. (A <em>big shout out to the </em><a href="https://github.com/SanderMertens/flecs"><em>flecs</em></a><em> team as their work on their </em><a href="https://github.com/flecs-hub/flecs-systems-admin"><em>admin panel</em></a><em> was a big inspiration for us when designing the UI</em>)</p><!--kg-card-begin: markdown--><ul>
<li><strong>(1) Main toolbar:</strong> You can control options that affect all the main panels. The buttons in order are:
<ul>
<li><strong>Entities (E), Components (C), Queries (Q) and Systems (S):</strong> the buttons will toggle the visibility for each of the main panels.</li>
<li><strong>Show relationship between components:</strong> When enabled it will highlight the relationships between Components, Queries and Systems. Eg: If you hover the <code>Rotating</code> component, it will highlight the <code>Rotating</code> query as it is using that component, as well as the <code>RotatingSystem</code>.</li>
<li><strong>Show debug:</strong> Show the JSON with all the stats and info collected from the ECSY application used by the extension.</li>
<li><strong>Show remote console (If debugging a remote device):</strong> It lets you enter commands to be executed on the remote device.</li>
<li><strong>Show Graphs:</strong> Toggle the graphs on all the sections.</li>
<li><strong>Show Stats:</strong> Toggle detailed stats on all the sections. This includes the following computed values for each entry:
<ul>
<li>Minimum</li>
<li>Maximum</li>
<li>Mean</li>
<li>Standard deviation</li>
</ul>
</li>
</ul>
</li>
<li><strong>(2) Version info:</strong> Shows the version info as well as the connection details if connected to a remote device.</li>
<li><strong>(3) Entities:</strong> The number of entities created.</li>
<li><strong>(4) Components:</strong> Shows the number of component classes and their instances in the application.
<ul>
<li>This section also includes a toolbar similar to the main one:
<ul>
<li><strong>Show graphs and stats:</strong> For all the components</li>
<li><strong>Link minimum and maximum values:</strong> It will synchronize all the graphs with the maximum and minimum globals values.</li>
<li><strong>Show pool size:</strong> It will include on each component’s graph another time series with the size of the pool for that component in red color.</li>
<li><strong>Show detailed stats:</strong> minimum, maximum, average and standard deviation.</li>
</ul>
</li>
<li>Then each component you can see:
<ul>
<li><strong>Tag components:</strong> They will include a special icon to easily spot them on the list.</li>
<li><strong>Pool detection:</strong>
<ul>
<li>If the component is not using a pool, it will show a warning icon indicating that it is recommended to define them as poolable for performance reasons.</li>
<li>Every time the application is creating a new component, you will see a flash message indicating if it has no pool, or the pool has no empty elements on it.</li>
</ul>
</li>
<li><strong>Dump to console:</strong> Each component has an arrow icon that will dump the list of instances of that component to the console. That list will also be assigned to a variable called <code>$component</code> so you can use it on the console.</li>
</ul>
</li>
</ul>
</li>
<li><strong>(5) Queries:</strong> Similar to the components section, here you also have a toolbar to toggle the graph and stats visibility or toggle the graphs’ minimum and maximum values synchronization.<br>
For each query you can see the components that take part in its definition and the number of entities the query has.</li>
<li><strong>(6) Systems:</strong> In this section you can explore all the systems being executed in your application loop and the timing for all of them.<br>
Each system shows the number of milliseconds it took to execute in the last frame, and the percentage of that time related to the whole frame time. For each query on the system you will also get extra stats if they are “reactive” (link ecsy), indicating the size of the queues for each event (added, removed or changed).<br>
We can control the execution for debugging or learning purposes:
<ul>
<li><strong>World:</strong> Applied to all systems:
<ul>
<li><strong>Play/Stop:</strong> Play or stop all the systems</li>
<li><strong>Step one frame:</strong> Execute all the systems one time</li>
<li><strong>Step next system:</strong> Execute the next system in order. Just one system at a time.</li>
</ul>
</li>
<li><strong>System</strong>
<ul>
<li><strong>Play/Stop:</strong> Play or stop the specific system</li>
<li><strong>Step:</strong> Execute the system one time.</li>
<li><strong>Solo:</strong> Pauses all the other systems but this one. Clicking on it again will undo the action.</li>
</ul>
</li>
</ul>
</li>
</ul>
<!--kg-card-end: markdown--><h1 id="remote-connection">Remote connection</h1><p>When debugging applications sometimes we want to test our code on an external device like a mobile phone or a standalone VR headset. The problem is that custom WebExtensions are not available when connecting to remote devices from the browser’s developer tools.</p><p>To address this issue we implemented a custom remote protocol using WebSockets and WebRTC, so you can connect to your device even if it is not connected via USB to your laptop, or using the same browser as you are using for debugging (eg: you could use Chrome to connect to a Firefox Reality browser on a VR device or Firefox to connect to an Oculus Browser sessions).</p><h2 id="how-to-enable-the-developer-tools-remote-connection-in-an-ecsy-application">How to enable the developer tools remote connection in an ECSY application?</h2><p>Today we also release a <a href="https://github.com/MozillaReality/ecsy">new version of ECSY (v.0.2)</a> that includes support for using the developer tools with remote devices.</p><p>Remote connection can be enabled using two methods:</p><!--kg-card-begin: markdown--><ul>
<li>
<p>Explicitly calling <code>enableRemoteDevtools()</code> on your application.</p>
<pre><code class="language-javascript">import {enableRemoteDevtools} from “ecsy”;
enableRemoteDevtools( /* optional Code */);
</code></pre>
<p>Please notice that you can pass an optional parameter as the code that the application will be listening for. Otherwise, a new one will be generated:</p>
</li>
<li>
<p>Adding <code>?enable-remote-devtools</code> to the URL when running your application on the browser you want to connect to.<br>
Using any of the previous methods will add an overlay on your application showing a random code that will be used to connect to remotely.</p>
</li>
</ul>
<!--kg-card-end: markdown--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh4.googleusercontent.com/M2cnkTe9tnlUijKyknSkm72R2Lss1kx90kNt9Qsn9SD-0R3YGbpR_Me1er8gNKP206fup3hGtZd6Sxz1fTEOMratGNh52QVJtTdKYXM-qNpLntVCXWqLM9mijcMZfIm9rI3zInpt" class="kg-image" alt="ECSY Developer tools extension"></figure><!--kg-card-end: image--><!--kg-card-begin: markdown--><h2 id="howtoconnectaremotedevice">How to connect a remote device?</h2>
<p>Once we get the code from the previous section, we can connect to that device from our preferred browser by clicking the ECSY icon on the extensions toolbar, entering the code and clicking <code>connect</code>:</p>
<!--kg-card-end: markdown--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh6.googleusercontent.com/MH_1hZvvMlm9DNIW_r79LbH0LTBizBWq3iLCbULSnl38LkIIFMS3n4RYnw02pBrFvpxessCEDK4hQJ9HEKY7UUBn6iEnGhr8No7KAA-prEiQz8pSqFO9AokW81wFjvMZvbqe-bDE" class="kg-image" alt="ECSY Developer tools extension"></figure><!--kg-card-end: image--><!--kg-card-begin: markdown--><p>Or doing the same from the <code>ECSY</code> panel in the developer tools.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh6.googleusercontent.com/100AlKXqjLuXE9ONm131myW4o8zsfrvb0cW6NIYl9NbvRWmXE7qkYT91akxlShYKBp883MC5ogyfidvHiIvdodjKR1D9bk7w7IreTBD2d8SBd3GI_Nw_7dM7Hzg2evYkSYj7u__P" class="kg-image" alt="ECSY Developer tools extension"></figure><!--kg-card-end: image--><!--kg-card-begin: markdown--><h2 id="featuresonremotedevices">Features on remote devices</h2>
<p>When using the developer tools on a remote device there are some differences from running it locally.</p>
<h3 id="consoleloganderrorshooks">Console.log and errors hooks</h3>
<p>We hook every call to <code>console.*</code> as well as any error thrown by the application running on the remote device, we will send them back to the host developer tools browser and log them on the console.</p>
<h3 id="dumptoconsole">Dump to console</h3>
<p>When connected to a remote device you still have the <code>dump to console</code> functionality to inspect components and queries, but it works slightly differently: The remote device will serialize the data and send it to the host developer tools that will deserialize it again and call <code>console.log</code> with that.<br>
Note that currently the deserialization process is quite simple, so we are just preserving the raw data, but we lose all functions and data type definitions.</p>
<h3 id="remoteconsole">Remote console</h3>
<p>We also included the ability to execute code on the remote device. Clicking on the <code>remote console</code> icon, the following panel will appear:</p>
<!--kg-card-end: markdown--><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://lh4.googleusercontent.com/xFVGbqK7tpQsPFS7h_2X4zd6cbWEjX_DbLKPd4nYbhB0vN51DSTEdA3JfitpD7Dzykeash486Ce2O3X4QVwYggssOSDjKClYCAriVwcej0dgoXOrloJSOL_sjR7ELZU_52dxB5XI" class="kg-image" alt="ECSY Developer tools extension"></figure><!--kg-card-end: image--><p>Everything you enter on the <em>textbox</em> will get executed on the remote device and it will write the result of the evaluation on the list above.</p><h1 id="what-s-next">What’s next?</h1><p>Currently, the extension is <em>still experimental</em> and we still have to fix issues on the current implementation (eg: right now there is a lot of re-render going on on the react UI making it slow on applications with many components and systems), and many other cool features we would like to include on the incoming version.</p><p>We would love your feedback! Please join us on <a href="https://discourse.mozilla.org/c/mixed-reality/ecsy">Discourse</a> or <a href="https://github.com/mozillareality/ecsy-devtools">Github</a> to discuss what you do think and share your needs.</p>]]></content:encoded></item><item><title><![CDATA[A Year with Spoke: Announcing the Architecture Kit]]></title><description><![CDATA[<p><a href="https://hubs.mozilla.com/spoke">Spoke, our 3D editor</a> for creating environments for <a href="https://hubs.mozilla.com/">Hubs</a>, is celebrating its first birthday with a major update. Last October, <a href="https://blog.mozvr.com/introducing-spoke/">we released the first version of Spoke</a>, a compositing tool for mixing 2D and 3D content to create immersive spaces. Over the past year, we’ve made a lot of</p>]]></description><link>https://blog.mozvr.com/year-with-spoke-architecture-kit/</link><guid isPermaLink="false">5da8ae14880eef0038aebbbf</guid><category><![CDATA[Tools]]></category><category><![CDATA[hubs]]></category><category><![CDATA[WebVR]]></category><category><![CDATA[social]]></category><dc:creator><![CDATA[Liv Erickson]]></dc:creator><pubDate>Tue, 29 Oct 2019 16:52:00 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/10/ArchKitBlogHeader.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/10/ArchKitBlogHeader.jpg" alt="A Year with Spoke: Announcing the Architecture Kit"><p><a href="https://hubs.mozilla.com/spoke">Spoke, our 3D editor</a> for creating environments for <a href="https://hubs.mozilla.com/">Hubs</a>, is celebrating its first birthday with a major update. Last October, <a href="https://blog.mozvr.com/introducing-spoke/">we released the first version of Spoke</a>, a compositing tool for mixing 2D and 3D content to create immersive spaces. Over the past year, we’ve made a lot of improvements and added new features to make building scenes for VR easier than ever. Today, we’re excited to share the latest feature that adds to the power of<a href="https://hubs.mozilla.com/spoke/projects"> Spoke: the Architecture Kit</a>!</p><p>We first talked about the <a href="https://blog.mozvr.com/building-mixed-reality-spaces-for-the-web/">components of the Architecture Kit back in March</a>. With the Architecture Kit, creators now have an additional way to build custom content for their 3D scenes without using an external tool. Specifically, we wanted to make it easier to take existing components that have already been optimized for VR and make it easy to configure those pieces to create original models and scenes. The Architecture Kit contains over 400 different pieces that are designed to be used together to create buildings - the kit includes wall, floor, ceiling, and roof pieces, as well as windows, trim, stairs, and doors. </p><!--kg-card-begin: embed--><figure class="kg-card kg-embed-card"><iframe src="https://player.vimeo.com/video/369589878?app_id=122963" width="426" height="240" frameborder="0" allow="autoplay; fullscreen" allowfullscreen title="Spoke Architecture Kit"></iframe></figure><!--kg-card-end: embed--><p>Because Hubs runs across mobile, desktop, and VR devices and delivered through the browser, performance is a key consideration. The different components of the Architecture Kit were created in <a href="https://blender.org">Blender </a>so that each piece would align together to create seamless connections. By avoiding mesh overlap, which is a common challenge when building with pieces that were made separately, z-fighting between faces becomes less of a problem. Many of the Architecture Kit pieces were made single-sided, which reduces the number of faces that need to be rendered. This is incredibly useful when creating interior or exterior pieces where one side of a wall or ceiling piece will never be visible by a user.</p><p>We wanted the Architecture Kit to be configurable beyond just the meshes. Buildings exist in a variety of contexts, so different pieces of the Kit can have one or more material slots with unique textures and materials that can be applied. This allows you to customize a wall with a window trim to have, for example, a brick wall and a wood trim. You can choose from the built-in textures of the Architecture Kit pieces directly in Spoke, <a href="https://github.com/MozillaReality/hubs-architecture-kit">or download the entire kit from GitHub</a>.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://blog.mozvr.com/content/images/2019/10/CoastalCliffHouseShot1.jpg" class="kg-image" alt="A Year with Spoke: Announcing the Architecture Kit"></figure><!--kg-card-end: image--><p>This release, which focuses on classic architectural styles and interior building tools, is just the beginning. As part of building the architecture kit,<a href="https://github.com/MozillaReality/hubs-blender-exporter"> we’ve built a Blender add-on</a> that will allow 3D artists to create their own kits. Creators will be able to specify individual collections of pieces and set an array of different materials that can be applied to the models to provide variation for different meshes. If you’re an artist interested in contributing to Spoke and Hubs by making a kit, drop us a line at <a href="mailto:hubs@mozilla.com">hubs@mozilla.com</a>.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://blog.mozvr.com/content/images/2019/10/CoastalCliffHouseShot2.jpg" class="kg-image" alt="A Year with Spoke: Announcing the Architecture Kit"></figure><!--kg-card-end: image--><p>In addition to the Architecture Kit, Spoke has had some other major improvements over the course of its first year. Recent changes to object placement have made it easier when laying out scene objects, and an update last week introduced the ability to <a href="https://hubs.mozilla.com/spoke/whats-new">edit multiple objects at one time</a>. We <a href="https://blog.mozvr.com/a-summer-with-particles-and-emojis/">added particle systems over the summer</a>, enabling more dynamic elements to be placed in your rooms. It’s also easier to visualize different components of your scene with a render mode toggle that allows you to swap between wireframe, normals, and shadows. The Asset Panel got a makeover, multi-select and edit was added, and we fixed a huge list of bugs when we made <a href="https://blog.mozvr.com/spoke-3d-scenes-web/">Spoke available as a fully hosted web app</a>.</p><p>Looking ahead to next year, we’re planning on adding features that will give creators more control over interactivity within their scenes. While the complete design and feature set for this isn’t yet fully scoped, we’ve gotten great feedback from Spoke creators that they want to add custom behaviors, so we’re beginning to think about what it will look like to experiment with scripting or programmed behavior on elements in a scene. If you’re interested in providing feedback and following along with the planning, we encourage you to join the <a href="https://discordapp.com/invite/wHmY4nd">Hubs community Discord server</a>, and keep an eye on the Spoke channel. You can also <a href="https://github.com/mozilla/spoke">follow development of Spoke on GitHub</a>.</p><p>There has never been a better time to start creating 3D content for the web. With new tools and features being added to Spoke all the time, we want your feedback in and we’d love to see what you’re building! Show off your creations and tag us on <a href="https://twitter.com/byhubs">Twitter @ByHubs</a>, or join us in the <a href="https://discord.gg/wHmY4nd">Hubs community Discord</a> and meet other users, chat with the team, and stay up to date with the latest in <a href="https://hubs.mozilla.com/">Mozilla Social VR</a>.</p>]]></content:encoded></item><item><title><![CDATA[Firefox Reality Top Picks - Bringing You New Virtual Reality Experiences Weekly]]></title><description><![CDATA[<p>So you bought yourself a fancy VR headset, you’ve played all the zombie-dragon-laser-kitten-battle games (we have too!) and now you’re wondering… what else is there? Where can I find other cool stuff to explore while I have this headset strapped to my face? We felt the same way,</p>]]></description><link>https://blog.mozvr.com/firefox-reality-top-picks/</link><guid isPermaLink="false">5da60bf3880eef0038aebb2b</guid><dc:creator><![CDATA[Kim Voynar]]></dc:creator><pubDate>Fri, 18 Oct 2019 17:54:54 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/10/toppicks.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/10/toppicks.png" alt="Firefox Reality Top Picks - Bringing You New Virtual Reality Experiences Weekly"><p>So you bought yourself a fancy VR headset, you’ve played all the zombie-dragon-laser-kitten-battle games (we have too!) and now you’re wondering… what else is there? Where can I find other cool stuff to explore while I have this headset strapped to my face? We felt the same way, so we built Firefox Reality to help you in your quest for the most interesting, groundbreaking and entertaining virtual reality content on the Web.</p><p>The real promise of VR is the ability to immerse yourself into countless other places and perspectives - both real and imaginary -  and to experience things you’ve never done before. Our <em>Top Picks</em> page is a great place to start exploring, with fresh recommendations coming weekly so you always have new content to check out. Of course, if you want to explore on your own, you can use Firefox Reality for that too.</p><p>Firefox Reality <em>Top Picks</em> is the start of what we hope will evolve into a thriving and sustainable ecosystem connecting creators, VR content, and audience.</p><h3 id="how-do-we-pick-our-top-picks-">How Do We Pick Our “Top Picks”?</h3><p><br>Unlike browsers that recommend content by mining your data and using AI, the content featured in the Firefox Reality <em>Top Picks</em> menu is curated by real humans. We build relationships with creator communities and scour the Web seeking the best experiences we can find from around the world. We keep our finger firmly on the pulse of what’s hottest, freshest and most engaging in the rapidly changing world of emerging tech content.</p><p>We seek out creators where they tend to congregate: at conferences, festivals, meetups and hackathons, on LinkedIn and in creator / developer Facebook and Reddit groups, and through artist networks. We also dig around the vast reaches of the Web and spend countless hours in headset watching and evaluating virtual reality videos and interacting with experiences, discovering first-hand what we need to have warnings for (like motion sickness, phobias, strong language or potentially triggering subject matter), so that you know before you dive in what you’re going into.</p><p>There are certain things we’re looking for, such as the quality of the video, editing, use of animation or special effects, the presence or absence of major technical flaws, and whether “best practices” for shooting and editing 360 video or building an interactive experience have been followed. If best practices aren’t followed, we like to see they’re being broken for a reason. We’re excited by new ways of storytelling, interesting ways to explore familiar places and concepts, simple-but-effective interactive games and experiences that can be played by anyone right over the Web.</p><p>Along with technical quality, we’re interested in the creative aspects of the work - the concept, the story, the theme, the characters and so on. But when evaluating immersive content, there’s another layer -  how the creator has made use of immersivity and/or interactivity. Does this feel like a story or experience that was specifically created for 360 space? Are the creators using traditional aspects of storytelling/journalism/art/music to do something new or different? Did the concept <em>have</em> to be told in 360 space or require interactivity to be effective?</p><p>For example:  The French piece <em>Bebe Moche</em>, featured currently in 360 Perspectives, tackles traditional physical slapstick comedy in 360 space. The French comedy troupe behind <em>Bebe Moche</em> have a whole series of short comedic “sketch” videos like this, and we’ll be showcasing them in <em>Top Picks</em> menu. Featuring the same cast and exploring physical comedy, they are a simple, effective experiments in physical storytelling that transcends verbal language. Perhaps equally important, it’s the kind of 360 video project that anyone could tackle with a decent 360 camera and video editing software.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://blog.mozvr.com/content/images/2019/10/org.mozilla.vrbrowser-20191015-123039.jpg" class="kg-image" alt="Firefox Reality Top Picks - Bringing You New Virtual Reality Experiences Weekly"></figure><!--kg-card-end: image--><p></p><p>Both content creators and audience are a key part of our journey as we work to make Firefox Reality a must-have tool for discovering, experiencing and sharing virtual reality content. If you are a creator interested in making a 360 video or interactive experience, or you want to know if the work you’re already making is WebXR-compatible, make sure to check out our <em>Immersive Media Content Guide</em>; it’s a great starting point for understanding more about how to make and share your own work.</p><p>If you have or know of an amazing 360 video or interactive experience you’d like our team to consider featuring on Firefox Reality <em>Top Picks</em>, please submit it to us <a href="https://qsurvey.mozilla.com/s3/Top-Picks">here</a>.</p><p>Firefox Reality 5 is available now. You can see our most recent release notes <a href="https://github.com/MozillaReality/FirefoxReality/wiki/Release-notes-for-version-5">here</a>. Go and get it!</p><p><a href="https://www.oculus.com/experiences/go/2208418715853974/"><strong>Download for Oculus Go</strong></a><br><a href="https://www.oculus.com/experiences/quest/2180252408763702"><strong>Download for Oculus Quest</strong></a><br><a href="https://www.viveport.com/mobileapps/05634fed-6dc5-4aa8-865d-af6027f4ec09"><strong>Download for Viveport</strong></a></p>]]></content:encoded></item><item><title><![CDATA[Introducing ECSY: an Entity Component System framework for the Web]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>Today we are introducing <a href="https://ecsy.io">ECSY</a> (Pronounced “eck-see”): a new -<em>highly experimental</em>- Entity Component System framework for Javascript.</p>
<p>After working on many interactive graphics projects for the web in the last few years we were trying to identify the common issues when developing something bigger than a simple example.</p>]]></description><link>https://blog.mozvr.com/introducing-ecsy/</link><guid isPermaLink="false">5d952b3dbaa78f0037b25d8f</guid><category><![CDATA[ECSY]]></category><dc:creator><![CDATA[Fernando Serrano]]></dc:creator><pubDate>Thu, 03 Oct 2019 18:23:34 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/10/header.jpg" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://blog.mozvr.com/content/images/2019/10/header.jpg" alt="Introducing ECSY: an Entity Component System framework for the Web"><p>Today we are introducing <a href="https://ecsy.io">ECSY</a> (Pronounced “eck-see”): a new -<em>highly experimental</em>- Entity Component System framework for Javascript.</p>
<p>After working on many interactive graphics projects for the web in the last few years we were trying to identify the common issues when developing something bigger than a simple example.<br>
Based on our findings we discussed what an ideal framework would need:</p>
<ul>
<li><strong>Component-based:</strong> Help to structure and reuse code across multiple projects.</li>
<li><strong>Predictable:</strong> Avoids random events or callbacks interrupting the main flow, which would make it hard to debug or trace what is going on in the application.</li>
<li><strong>Good performance:</strong> Most web graphics applications are CPU bound, so we should focus on performance much more.</li>
<li><strong>Simple API:</strong> The core API should be simple, making the framework easier to understand, optimize and contribute to; but also allow building more complex layers on top of it if needed.</li>
<li><strong>Graphics engine agnostic:</strong> It should not be tied to any specific graphics engine or framework.</li>
</ul>
<p>These requirements are high-level features that are not usually provided by graphics engines like <a href="https://threejs.org">three.js</a> or <a href="https://www.babylonjs.com">babylon.js</a>. On the other hand, <a href="https://aframe.io">A-Frame</a> provides a nice component-based architecture, which is really handy when developing bigger projects, but it lacks the rest of the previously mentioned features. For example:</p>
<ul>
<li><strong>Performance:</strong> Dealing with the DOM implies overhead. Although we have been building <code>A-Frame</code> applications with good performance, this could be done by breaking the API contract, for example by accessing the values of the components directly instead of using <a href="https://aframe.io/docs/0.9.0/core/entity.html#setattribute-componentname-value-propertyvalue-clobber">setAttribute/getAttribute</a>. This can lead to some unwanted side effects, such as incompatibility between components and a lack of reactive behavior.</li>
<li><strong>Predictable:</strong> Dealing with asynchrony because of the DOM life cycle or the events’ callbacks when modifying attributes makes the code really hard to debug and to trace.</li>
<li><strong>Graphics engine agnostic:</strong> Currently <code>A-Frame</code> and its components are so strongly tied to Three.js that it makes no sense to change it to any other engine.</li>
</ul>
<p>After analyzing these points, gathering our experience with <code>three.js</code> and <code>A-Frame</code>, and looking at the state of the art on game engines like <a href="https://unity.com">Unity</a>, we decided to work on building this new framework using a pure <a href="https://en.wikipedia.org/wiki/Entity_component_system">Entity Component System</a> architecture. The difference between a pure ECS like <a href="https://unity.com/dots">Unity DOTS</a>, <a href="https://github.com/skypjack/entt">entt</a>, or <a href="https://github.com/sschmid/Entitas-CSharp">Entitas</a>, and a more object oriented approach, such as <a href="https://docs.unity3d.com/ScriptReference/MonoBehaviour.html">Unity’s MonoBehaviour</a> or <a href="https://aframe.io/docs/0.9.0/core/component.html">A-Frame's Components</a>, is that in the latter the components and systems both have logic and data, while with a pure ECS approach components just have data (without logic) and the logic resides in the systems.</p>
<p>Focusing on building a simple core for this new framework helps iterate faster when developing new applications and lets us implement new features on top of it as needed. It also allows us to use it with existing libraries as <a href="https://threejs.org/">three.js</a>, <a href="https://www.babylonjs.com/">Babylon.js</a>, <a href="https://phaser.io/">Phaser</a>, <a href="https://www.pixijs.com/">PixiJS</a>, interacting directly with the DOM, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API">Canvas</a> or <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API">WebGL</a> APIs, or prototype around new APIs as <a href="https://github.com/gpuweb/gpuweb">WebGPU</a>, <a href="https://webassembly.org">WebAssembly</a> or <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">WebWorkers</a>.</p>
<p style="text-align: center">
    <img style="margin-bottom: 0px" src="https://blog.mozvr.com/content/images/2019/10/Stacks.svg" alt="Introducing ECSY: an Entity Component System framework for the Web">
    <span><em>Technology stack examples</em></span>
</p>
<h2 id="architecture">Architecture</h2>
<p>We decided to use a <a href="https://en.wikipedia.org/wiki/Data-oriented_design">data-oriented architecture</a> as we noticed that having data and logic separated helps us better think about the structure of our applications. This also allows us to work internally on optimizations, for example how to store and access this data or how to get the advantage of parallelism for the logic.</p>
<p>The terms you must know in order to work with our framework are mostly the same as any other ECS:</p>
<ul>
<li><strong>Entities:</strong> Empty objects with unique IDs that can have multiple components attached to it.</li>
<li><strong>Components:</strong> Different facets of an entity. ex: geometry, physics, hit points. Components only store data.</li>
<li><strong>Systems:</strong> Where the logic is. They do the actual work by processing  entities and modifying their components. They are data processors, basically.</li>
<li><strong>Queries:</strong> Used by systems to determine which entities they are interested in, based on the components the entities own.</li>
<li><strong>World:</strong> A container for entities, components, systems, and queries.</li>
</ul>
<p style="text-align: center">
    <img style="margin-bottom: 0px" src="https://blog.mozvr.com/content/images/2019/10/ECSY-Architecture.svg" alt="Introducing ECSY: an Entity Component System framework for the Web">
    <span><em>ECSY Architecture</em></span>
</p>
<h2 id="example">Example</h2>
<p>So far all the information has been quite abstract, so let’s dig into a simple example to see how the API feels.</p>
<p>The example will consist of boxes and circles moving around the screen, nothing fancy but enough to understand how the API works.</p>
<p>We will start by defining components that will be attached to the entities in our application:</p>
<ul>
<li><strong>Position:</strong> The position of the entity on the screen.</li>
<li><strong>Velocity:</strong> The speed and direction in which the entity moves.</li>
<li><strong>Shape:</strong> The type of shape the entity has: <code>circle</code> or <code>box</code>.<br>
Now we will define the systems that will hold the logic in our application:</li>
<li><strong>MovableSystem:</strong> It will look for entities that have <code>speed</code> and <code>position</code> and it will update their <code>position</code> component.</li>
<li><strong>RendererSystem:</strong> It will paint the shapes at their current position.</li>
</ul>
<p style="text-align: center">
    <img style="margin-bottom: 0px" src="https://blog.mozvr.com/content/images/2019/10/Circles-and-balls-example-1.svg" alt="Introducing ECSY: an Entity Component System framework for the Web">
    <span><em>Circles and balls example design</em></span>
</p>
<p>Below is the code for the example described, some parts have been omitted to abbreviate (Check the full source code on <a href="https://github.com/MozillaReality/ecsy/blob/dev/examples/circles-boxes/index.html">Github</a> or <a href="https://glitch.com/~ecsy-boxes-and-circles">Glitch</a>)</p>
<!-- Copy and Paste Me -->
<div class="glitch-embed-wrap" style="height: 420px; width: 100%;">
  <iframe src="https://glitch.com/embed/#!/embed/ecsy-boxes-and-circles?path=index.html&previewSize=100" title="ecsy-boxes-and-circles on Glitch" allow="geolocation; microphone; camera; midi; vr; encrypted-media" style="height: 100%; width: 100%; border: 0;">
  </iframe>
</div>
<br>
<p>We start by defining the components we will be using:</p>
<pre><code class="language-javascript">// Velocity component
class Velocity {
  constructor() {
    this.x = this.y = 0;
  }
}

// Position component
class Position {
  constructor() {
    this.x = this.y = 0;
  }
}

// Shape component
class Shape {
  constructor() {
    this.primitive = 'box';
  }
}

// Renderable component
class Renderable extends TagComponent {}
</code></pre>
<p>Now we implement the two systems our example will use:</p>
<pre><code class="language-javascript">// MovableSystem
class MovableSystem extends System {
  // This method will get called on every frame by default
  execute(delta, time) {
    // Iterate through all the entities on the query
    this.queries.moving.results.forEach(entity =&gt; {
      var velocity = entity.getComponent(Velocity);
      var position = entity.getMutableComponent(Position);
      position.x += velocity.x * delta;
      position.y += velocity.y * delta;

      if (position.x &gt; canvasWidth + SHAPE_HALF_SIZE) position.x = - SHAPE_HALF_SIZE;
      if (position.x &lt; - SHAPE_HALF_SIZE) position.x = canvasWidth + SHAPE_HALF_SIZE;
      if (position.y &gt; canvasHeight + SHAPE_HALF_SIZE) position.y = - SHAPE_HALF_SIZE;
      if (position.y &lt; - SHAPE_HALF_SIZE) position.y = canvasHeight + SHAPE_HALF_SIZE;
    });
  }
}

// Define a query of entities that have &quot;Velocity&quot; and &quot;Position&quot; components
MovableSystem.queries = {
  moving: {
    components: [Velocity, Position]
  }
}

// RendererSystem
class RendererSystem extends System {
  // This method will get called on every frame by default
  execute(delta, time) {

    ctx.globalAlpha = 1;
    ctx.fillStyle = &quot;#ffffff&quot;;
    ctx.fillRect(0, 0, canvasWidth, canvasHeight);

    // Iterate through all the entities on the query
    this.queries.renderables.results.forEach(entity =&gt; {
      var shape = entity.getComponent(Shape);
      var position = entity.getComponent(Position);
      if (shape.primitive === 'box') {
        this.drawBox(position);
      } else {
        this.drawCircle(position);
      }
    });
  }
  // drawCircle and drawCircle hidden for simplification
}

// Define a query of entities that have &quot;Renderable&quot; and &quot;Shape&quot; components
RendererSystem.queries = {
  renderables: { components: [Renderable, Shape] }
}
</code></pre>
<p>We create a world and register the systems that it will use:</p>
<pre><code class="language-javascript">var world = new World();
world
  .registerSystem(MovableSystem)
  .registerSystem(RendererSystem);
</code></pre>
<p>We create some entities with random position, speed, and shape.</p>
<pre><code class="language-javascript">for (let i = 0; i &lt; NUM_ELEMENTS; i++) {
  world
    .createEntity()
    .addComponent(Velocity, getRandomVelocity())
    .addComponent(Shape, getRandomShape())
    .addComponent(Position, getRandomPosition())
    .addComponent(Renderable)
}
</code></pre>
<p>Finally, we just have to update it on each frame:</p>
<pre><code class="language-javascript">function run() {
  // Compute delta and elapsed time
  var time = performance.now();
  var delta = time - lastTime;

  // Run all the systems
  world.execute(delta, time);

  lastTime = time;
  requestAnimationFrame(run);
}

var lastTime = performance.now();
run();
</code></pre>
<h2 id="features">Features</h2>
<p>The main features that the framework currently has are:</p>
<ul>
<li><strong>Engine/framework agnostic:</strong> You can use ECSY directly with whichever 2D or 3D engine you are already used to. We have provided some simple examples for <code>Babylon.js</code>, <code>three.js</code>, and <code>2D canvas</code>. To make things even easier, we plan to release a set of bindings and helper components for the most commonly used engines, starting with three.js.</li>
<li><strong>Focused on providing a simple, yet efficient API:</strong> We want to make sure to keep the API surface as small as possible, so that the core remains simple and is easy to maintain and optimize. More advanced features can be layered on top, rather than being baked into the core.</li>
<li><strong>Designed to avoid garbage collection as much as possible:</strong> It will try to use pools for entities and components whenever possible, so objects won’t be allocated when adding new entities or components to the world.</li>
<li><strong>Systems, entities, and components are scoped in a “world” instance:</strong> It means that we don’t register the components or systems on the global scope, allowing you to have multiple worlds or apps running on the same page without interferences between them.</li>
<li><strong>Multiple queries per system:</strong> You can define as many queries per system as you want.</li>
<li><strong>Reactive support:</strong>
<ul>
<li>Systems can react to changes in the entities and components</li>
<li>Systems can get mutable or immutable references to components on an entity.</li>
</ul>
</li>
<li><strong>Predictable:</strong>
<ul>
<li>Systems will always run in the order they were registered or based on a priority attribute.</li>
<li>Reactive events won’t generate “random” callbacks when emitted. Instead they will be queued and processed in order, when the listener systems are executed.</li>
</ul>
</li>
<li><strong>Modern Javascript:</strong> ES6, classes, modules</li>
</ul>
<h2 id="whatsnext">What’s next?</h2>
<p>This project is still in its early days so you can expect a lot of changes with the API and many new features to arrive in the upcoming weeks. Some of the ideas we would like to work on are:</p>
<ul>
<li><strong>Syntactic sugar:</strong> As the API is still evolving we have not focused on adding a lot of syntactic sugar, so currently there are places where the code is very verbose.</li>
<li><strong>Developer tools:</strong> In the coming weeks we plan to release a developer tools extension to visualize the status of the ECS on your application and help you debug and understand its status.</li>
<li><strong>ecsy-three:</strong> As discussed previously ecsy is engine-agnostic, but we will be working on providing bindings for commonly used engines starting with three.js.</li>
<li><strong>Declarative layer:</strong> Based on our experience working with A-Frame, we understand the value of having a declarative layer so we would like to experiment with that on ECSY too.</li>
<li><strong>More examples:</strong> Keep using a diverse range of underlying APIs, such as canvas, WebGL and engines like three.js, babylon.js , etc.</li>
<li><strong>Performance:</strong> We have not really dug into optimizations on the core and we plan to look into it in the upcoming weeks and we will be publishing some benchmarking and results. The main goal of this initial release was to have an API we like so we could then focus on the core in order to make it run as fast as possible.<br>
You may notice that ECSY is not focused on data-locality or memory layout as much as many native ECS engines. This has not been a priority in ECSY because in Javascript we have far less control over the way things are laid out in memory and how our code gets executed on the CPU. We get far bigger wins by focusing on preventing unnecessary garbage collection and optimizing for JIT. This story will change quite a bit with WASM, so it is certainly something we want to explore for ECSY in the future.</li>
<li><strong>WASM:</strong> We want to try to implement parts of the core or some systems on WASM to take advantage of strict memory layout and parallelism by using WASM threads and SharedArrayBuffers.</li>
<li><strong>WebWorkers:</strong> We will be working on examples showing how you can move systems to a worker to run them in parallel.</li>
</ul>
<p>Please feel free to use <a href="https://github.com/MozillaReality/ecsy">Github</a> to follow the development, request new features or file issues on bugs you find, our <a href="https://discourse.mozilla.org/c/mixed-reality/ecsy">discourse forum</a> to discuss how to use ecsy on your projects, and <a href="https://ecsy.io">ecsy.io</a> for more examples and documentation.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Virtual identities in Hubs]]></title><description><![CDATA[Identity is a complicated concept—who are we really? Virtual spaces make this even harder. Having control over our representation and identity online is a critical component of safety and privacy, and platforms should prioritize user agency.]]></description><link>https://blog.mozvr.com/virtual-identities-in-hubs/</link><guid isPermaLink="false">5d82bd9b3016080037792adb</guid><category><![CDATA[hubs]]></category><category><![CDATA[privacy]]></category><category><![CDATA[social]]></category><category><![CDATA[vr]]></category><dc:creator><![CDATA[Diane Hosfelt]]></dc:creator><pubDate>Thu, 19 Sep 2019 13:00:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1485550409059-9afb054cada4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1485550409059-9afb054cada4?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Virtual identities in Hubs"><p>Identity is a complicated concept—who are we really? Most of us have government IDs that define part of our identity, but that’s just a starting point. We present ourselves differently depending on context—who we are with our loved ones might not be the same as who we are at work, but both are legitimate representations of ourselves.</p><p>Virtual spaces make this even harder. We might maintain many virtual identities with different degrees of overlap. Having control over our representation and identity online is a critical component of safety and privacy, and platforms should prioritize user agency.</p><p>More importantly, autonomy and privacy are intrinsically intertwined. If everyone saw my google searches, I would probably change what I search for. If I knew my employer could monitor my interactions when I’m not at work, I would behave differently. Privacy isn’t just about protecting information about myself, it’s about allowing me to express myself.</p><h3 id="avatars">Avatars</h3><p>Avatars are a digital representation of individuals. They enable virtual embodiment, making communication in a virtual environment more natural and analogous to communication in real life. They also help us ground ourselves spatially in the 3D environment and allow others to have a specific point to reference, which enables directional sentiment and simulated eye contact.</p><p>Your decisions about avatar representation can both reveal personally identifiable information about you, such as your face and affect your self-perception and influence your behavior. This phenomenon is known as the <a href="https://vhil.stanford.edu/mm/2007/yee-proteus-effect.pdf" rel="noreferrer nofollow noopener">Proteus Effect</a>. This effect can induce societal biases, like feeling more confident when embodying taller avatars.</p><p>When we think about applying concepts related to identity to social VR platforms like Hubs, platforms need to design and implement features that focus on enabling users to easily manage their identities. On the avatar side, that means making it really easy to choose, change, and customize avatars so that you decide how much or little you want it to represent you.</p><p>In Hubs, we chose robots as the default avatar instead of picking a single human representation. However, any glb files <a href="http://​​https://github.com/MozillaReality/hubs-avatar-pipelines" rel="noreferrer nofollow noopener">can be used as the base avatar form</a> - it was important for us that the platform could support any number of avatar styles, driven by the communities and users themselves. That means if you prefer cats or pinecones, you have the flexibility to choose that representation for yourself instead.</p><p>When it comes to determining visual identity of one’s self in a 3D space, that control should belong to the user, not the platform. </p><h3 id="accounts">Accounts</h3><p>When you interact with others online, there is a risk of exposing different parts of your identity to them and it isn’t always clear what is exposed when you make an account on a website. Your profile on a social network, for example, may have an image of you shared with other people, display your legal name, or show an account pseudonym that you use with other online services. </p><p>Hubs allows you to use the platform regardless of whether or not you have an account, but one of the benefits of account-based services is the ability to have a known identity that can be responsible for certain actions and behaviors. Certain actions, like being promoted to a room moderator, require an account. A challenge with pseudonymous and anonymous spaces is that a lack of a valued account can also result in a lack of accountability.</p><p>Hubs accounts are purposely lightweight, requiring only an email address. Being able to tie your virtual identity to a second account, such as a Discord account, can provide further benefits, such as increased room security, and the ability to communicate across different platforms. Particularly when room links are more widely distributed, the room dynamics can benefit from linking users to a known identity— but platforms should respect how much information they request from users. </p><p>There’s a balance with how much knowledge the platforms need to validate identities and how much data users need to provide. When possible, personal information collection should be minimized.</p><p>Mozilla Hubs is an open source social VR platform—come try it out at <a href="https://hubs.mozilla.com">hubs.mozilla.com</a> or contribute <a href="https://github.com/mozilla/hubs">here</a>. Read more about privacy in Hubs <a href="https://blog.mozvr.com/creating-privacy-centric-virtual-spaces/">here</a>.</p>]]></content:encoded></item><item><title><![CDATA[Creating privacy-centric virtual spaces]]></title><description><![CDATA[As immersive devices become more affordable, social spaces in virtual reality (VR) will become more integrated into our daily lives. If social VR is the next evolution of this, what approaches will result in spaces that respect user identities, autonomy, and safety?]]></description><link>https://blog.mozvr.com/creating-privacy-centric-virtual-spaces/</link><guid isPermaLink="false">5d7962028b6ebd003834f43e</guid><category><![CDATA[hubs]]></category><category><![CDATA[privacy]]></category><category><![CDATA[vr]]></category><category><![CDATA[ethics]]></category><dc:creator><![CDATA[Diane Hosfelt]]></dc:creator><pubDate>Thu, 12 Sep 2019 21:02:25 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1506031765313-0bc574a405f0?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1506031765313-0bc574a405f0?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Creating privacy-centric virtual spaces"><p>We now live in a world with instantaneous communication unrestrained by geography. While a generation ago, we would be limited by the speed of the post, now we’re limited by the speed of information on the Internet. This has changed how we connect with other people.</p>
<p>As immersive devices become more affordable, social spaces in virtual reality (VR) will become more integrated into our daily lives and interactions with friends, family, and strangers. Social media has enabled rapid pseudonymous communication, which can be directed at both a single person and large groups. If social VR is the next evolution of this, what approaches will result in spaces that respect user identities, autonomy, and safety?</p>
<p>We need spaces that reflect how we interact with others on a daily basis.</p>
<h2 id="socialspacesirlandivr">Social spaces: IRL and IVR</h2>
<p>Often, when people think about social VR, what tends to come to mind are visions from the worlds of science fiction stories: Snow Crash, Ready Player One, The Matrix - huge worlds that involve thousands of strangers interacting virtually on a day to day basis. In today’s social VR ecosystem, many applications take a similarly public approach: new users are often encouraged (or forced) by the system to interact with new people in the name of developing relationships with strangers who are also participating in the shared world. This can result in more dynamic and populated spaces, but in a way that isn’t inherently understood from our regular interactions.</p>
<p>This approach doesn’t mirror our usual day-to-day experiences—instead of spending time with strangers, we mostly interact with people we know. Whether we’re in a private, semi-public, or public space, we tend to stick to familiarity. We can define the privacy of space by thinking about who has access to a location, and the degree to which there is established trust among other people you encounter there.</p>
<p><strong>Private</strong>: a controlled space where all individuals are known to each other. In the physical world, your home is an example of a private space—you know anyone invited into your home, whether they’re a close associate, or a passing acquaintance (like a plumber)<br>
<strong>Semi-public</strong>: a semi-controlled space where all individuals are associated with each other. For example, you might not know everyone in your workplace, but you’re all connected via your employer<br>
<strong>Public</strong>: a public space made up of a lot of different, separate groups of people who might not have established relationships or connections. In a restaurant, while you know the group you’re dining with, you likely don’t know anyone else</p>
<p><img src="https://blog.mozvr.com/content/images/2019/09/Hubsprivimg.jpg" alt="Creating privacy-centric virtual spaces"></p>
<p>While we might encounter strangers in public or semi-public spaces, most of our interactions are still with people we know. This should extend to the virtual world. However, VR devices haven’t been widely available until recently, so most companies building virtual worlds have designed their spaces in a way that prioritizes getting people in the same space, regardless of whether or not those users already know each other.</p>
<p>For many social VR systems, the platform hosting spaces often networks different environments and worlds together and provides a centralized directory of user-created content to go explore. While this type of discovery has benefits and values, in the physical world, we largely spend time with the same people from day to day. Why don’t we design a social platform around this?</p>
<p>Mozilla Hubs is a social VR platform created to provide spaces that more accurately emulate our IRL interactions. Instead of hosting a connected, open ecosystem, users create their own independent, private-by-default rooms. This creates a world where instead of wandering into others’ spaces, you intentionally invite people you know into your space.</p>
<h2 id="privatebydefault">Private by default</h2>
<p>Communities and societies often <a href="https://medium.com/@jessica.outlaw/want-social-norms-9-steps-to-building-a-strong-culture-part-3-d7da8bacf71e">establish their own cultural norms</a>, signals, inside jokes, and unspoken (or written) rules — these carry over to online spaces.  It can be difficult for people to be thrown into brand-new groups of users without this understanding, and there are often no guarantees that the people you’ll be interacting with in these public spaces will be receptive to other users who are joining. In contrast to these public-first platforms, we’ve designed our social VR platform, Hubs, to be private by default. This means that instead of being in an environment with strangers from the outset, Hubs rooms are designed to be private to the room owner, who can then choose who they invite into the space with the room access link.</p>
<h2 id="protectingpublicspaces">Protecting public spaces</h2>
<p>When we’re in public spaces, we have different sets of implied rules than the social norms that we might abide by when we’re in private. In virtual spaces, these rules aren’t always as clear and different people will behave differently in the absence of established rules or expectations. Hubs allows communities to set up their own public spaces, so that they can bring their own social norms into the spaces. When people are meeting virtually, it’s important to consider the types of interactions that you’re encouraging.</p>
<p>Because access to a Hubs room is predicated on having the invitation URL, the degree to which that link is shared by the room owner or visitors to the room will dictate how public or private a space is. If you know that the only people in a Hubs room are you and two of your closest friends, you probably have a pretty good sense of how the three of you interact together. If you’re hosting a meetup and expecting a crowd, those behaviors can be less clear. Without intentional community management practices, virtual spaces can turn ugly. Here are some things that you could consider to keep semi-public or public Hubs rooms safe and welcoming:</p>
<ul>
<li>Keep the distribution of the Hubs link limited to known groups of trusted individuals and consider using a form or registration to handle larger events.</li>
<li>Use an integration like the <a href="https://blog.mozvr.com/hubs-discord-beta/">Hubs Discord Bot</a> to back users against a known identity. Removing users from a linked Discord channel also removes their ability to enter an authenticated Hubs room.</li>
<li>Lock down room permissions in the room to limit who can create new objects in the room or draw with the pen tool.</li>
<li><a href="https://mozillascience.github.io/working-open-workshop/code_of_conduct/">Create a code of conduct</a> or set of community guidelines that are posted in the Hubs room near the room entry point.</li>
<li>Assign trusted users to act as <a href="https://github.com/mozilla/hubs/wiki/Moderating-Rooms">moderators</a> so users who is available in the space to help welcome visitors and enforce positive conduct.</li>
</ul>
<p>We need social spaces that respect and empower participants. Here at Mozilla, we’re creating a platform that more closely reflects how we interact with others IRL. Our spaces are private by default, and Hubs allows users to control who enters their space and how visitors can behave.</p>
<p>Mozilla Hubs is an open source social VR platform—come try it out at <a href="https://hubs.mozilla.com/">hubs.mozilla.com</a> or contribute <a href="https://github.com/mozilla/hubs">here</a>.</p>
<p>Read more about safety and identity in Hubs <a href="https://blog.mozvr.com/virtual-identities-in-hubs/">here</a>.</p>
]]></content:encoded></item><item><title><![CDATA[Multiview on WebXR]]></title><description><![CDATA[<p>The WebGL multiview extension is already available in several browsers and 3D web engines and it could easily help to improve the performance on your WebXR application</p>
<h2 id="whatismultiview">What is multiview?</h2>
<p>When VR first arrived, many engines supported stereo rendering by running all the render stages twice, one for each camera/</p>]]></description><link>https://blog.mozvr.com/multiview-on-webxr/</link><guid isPermaLink="false">5d7a88268b6ebd003834f451</guid><category><![CDATA[WebXR]]></category><category><![CDATA[Multiview]]></category><category><![CDATA[Performance]]></category><category><![CDATA[WebGL]]></category><dc:creator><![CDATA[Fernando Serrano]]></dc:creator><pubDate>Thu, 12 Sep 2019 18:36:30 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/09/multiview.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/09/multiview.png" alt="Multiview on WebXR"><p>The WebGL multiview extension is already available in several browsers and 3D web engines and it could easily help to improve the performance on your WebXR application</p>
<h2 id="whatismultiview">What is multiview?</h2>
<p>When VR first arrived, many engines supported stereo rendering by running all the render stages twice, one for each camera/eye. While it works it is highly inefficient.</p>
<pre><code class="language-javascript">for (eye in eyes)
	renderScene(eye)
</code></pre>
<p>Where <code>renderScene</code> will setup the viewport, shaders, and states every time is being called. This will double the cost of rendering every frame.</p>
<p>Later on, some optimizations started to appear in order to improve the performance and minimize the state changes.</p>
<pre><code class="language-javascript">for (object in scene) 
	for (eye in eyes)
		renderObject(object, eye)
</code></pre>
<p>Even if we reduce the number of state changes, by switching programs and grouping objects, the number of draw calls remains the same: two times the number of objects.</p>
<p>In order to minimize this bottleneck, <a href="https://www.khronos.org/registry/OpenGL/extensions/OVR/OVR_multiview.txt">the multiview extension was created</a>. The TL;DR of this extension is: Using just one drawcall you can draw on multiple targets, reducing the overhead per view.</p>
<p><img src="https://blog.mozvr.com/content/images/2019/09/framemultiview-1.png" alt="Multiview on WebXR"></p>
<p>This is done by modifying your shader uniforms with the information for each view and accessing them with the <code>gl_ViewID_OVR</code>, similar to how the <code>Instancing API</code> works.</p>
<pre><code class="language-c">in vec4 inPos;
uniform mat4 u_viewMatrices[2];
void main() {
    gl_Position = u_viewMatrices[gl_ViewID_OVR] * inPos;
}
</code></pre>
<p>The resulting render loop with the multiview extension will look like:</p>
<pre><code class="language-javascript">for (object in scene)
    setUniformsForBothEyes() // Left/Right camera matrices
    renderObject(object)
</code></pre>
<p>This extension can be used to improve multiple tasks as cascaded shadow maps, rendering cubemaps, rendering multiple viewports as in CAD software, although the most common use case is stereo rendering.</p>
<p>Stereo rendering is also our main target as this will improve the VR rendering path performance with just a few modifications in a 3D engine. Currently, most of the headsets have two views, but there are prototypes of headset with ultra-wide FOV using 4 views which is currently the maximum number of views supported by multiview.</p>
<h2 id="multiviewinwebgl">Multiview in WebGL</h2>
<p>Once the <a href="https://www.khronos.org/registry/OpenGL/extensions/OVR/OVR_multiview.txt">OpenGL OVR_multiview2 specification</a> was created, the WebGL working group started to make a WebGL version of this API.</p>
<p>It’s been a while since our <a href="https://blog.mozvr.com/multiview-servo-architecture/">first experiment supporting multiview on servo and three.js</a>. Back then it was quite a challenge to support WEBGL_multiview: it was based on opaque framebuffers and it was possible to use it with WebGL1 but the shaders need to be compiled with GLSL 3.0 support, which was only available on <code>WebGL2</code>, so some hacks on the servo side were needed in order to get it running.<br>
At that time the WebVR spec had a proposal to support multiview but it was not approved.</p>
<p>Thanks to the work of the WebGL WG, the multiview situation has improved a lot in the last few months. The specification is already in the Community Approved status, which means that browsers could ship it enabled by default (As we do on Firefox desktop 70 and Firefox Reality 1.4)</p>
<p>Some important restrictions of the final specification to notice:</p>
<ul>
<li>It only supports <code>WebGL2</code> contexts, as it needs <code>GLSL 3.00</code> and <code>texture arrays</code>.</li>
<li>Currently there is no way to use multiview to render to a multisampled backbuffer, so you should create contexts with <code>antialias: false</code>. (The WebGL WG <a href="https://github.com/KhronosGroup/WebGL/issues/2912">is working on a solution for this</a>)</li>
</ul>
<h2 id="webengineswithmultiviewsupport">Web engines with multiview support</h2>
<p>We have been working for a while on adding multiview support to <strong>three.js</strong> (<a href="https://github.com/mrdoob/three.js/pull/16316">PR</a>). Currently it is possible to get the benefits of multiview automatically as long as the extension is available and you define a <code>WebGL2</code> context without antialias:</p>
<pre><code class="language-javascript">var context = canvas.getContext( 'webgl2', { antialias: false } );
renderer = new THREE.WebGLRenderer( { canvas: canvas, context: context } );
</code></pre>
<p>You can see a three.js example using multiview <a href="https://raw.githack.com/mrdoob/three.js/dev/examples/webvr_multiview.html">here</a> (<a href="https://github.com/mrdoob/three.js/blob/dev/examples/webvr_multiview.html">source code</a>).</p>
<p><strong>A-Frame</strong> is based on <code>three.js</code> so they should get multiview support as soon as they update to the latest release.</p>
<p><strong>Babylon.js</strong> has had support for OVR_multiview2 already for a while (<a href="https://doc.babylonjs.com/how_to/multiview">more info</a>).</p>
<p>For details on how to use multiview directly without using any third party engine, you could take a look at the <a href="https://github.com/mrdoob/three.js/pull/16316">three.js implementation</a>, see the <a href="https://www.khronos.org/registry/webgl/extensions/OVR_multiview2/">specification’ sample code</a> or read <a href="https://developer.oculus.com/documentation/oculus-browser/latest/concepts/browser-multiview/">this tutorial</a> by Oculus.</p>
<h2 id="browserswithmultiviewsupport">Browsers with multiview support</h2>
<p>The extension was just approved by the Community recently so we expect to see all the major browsers adding support for it by default soon</p>
<ul>
<li><strong>Firefox Desktop</strong>: <a href="https://www.mozilla.org/en-US/firefox/">Firefox 71</a> will support multiview enabled by default. In the meantime, you can test it on <a href="https://www.mozilla.org/en-US/firefox/channel/desktop/">Firefox Nightly</a> by enabling draft extensions.</li>
<li><strong>Firefox Reality</strong>: It’s already enabled by default since <a href="https://mixedreality.mozilla.org/firefox-reality/">version 1.3</a>.</li>
<li><strong>Oculus Browser</strong>: It’s implemented but disabled by default, you must enable <code>Draft WebGL Extension</code> preference in order to use it.</li>
<li><strong>Chrome</strong>: You can use it on <a href="https://www.google.com/chrome/canary/">Chrome Canary</a> for Windows by running it with the following command line parameters: <code>--use-cmd-decoder=passthrough --enable-webgl-draft-extensions</code></li>
</ul>
<h2 id="performanceimprovements">Performance improvements</h2>
<p>Most WebGL or WebXR applications are CPU bound, the more objects you have on the scene the more draw calls you will submit to the GPU. In our benchmarks for stereo rendering with two views, <strong>we got a consistent improvement of ~40%</strong> compared to traditional rendering.<br>
As you can see on the following chart, the more cubes (drawcalls) you have to render, the better the performance will be.<br>
<img src="https://blog.mozvr.com/content/images/2019/09/performancechart.png" alt="Multiview on WebXR"></p>
<h2 id="whatsnext">What’s next?</h2>
<p>The main drawback when using the current multiview extension is that there is no way to render to a multisampling backbuffer. In order to use it with WebXR you should set <code>antialias: false</code> when creating the context. However this is something the WebGL WG <a href="https://github.com/KhronosGroup/WebGL/issues/2912">is working on</a>.</p>
<p>As soon as they come up with a proposal and is implemented by the browsers, 3D engines should support it automatically. Hopefully, we will see new extensions arriving to the WebGL and WebXR ecosystem to improve the performance and quality of the rendering, such as the ones exposed by <a href="https://developer.nvidia.com/vrworks">Nvidia VRWorks</a> (eg: <code>Variable Rate Shading</code> and <code>Lens Matched Shading</code>).</p>
<h2 id="references">References</h2>
<p><a href="https://developer.nvidia.com/vrworks/graphics/multiview">https://developer.nvidia.com/vrworks/graphics/multiview</a><br>
<a href="https://developer.oculus.com/documentation/mobilesdk/latest/concepts/mobile-multiview/">https://developer.oculus.com/documentation/mobilesdk/latest/concepts/mobile-multiview/</a><br>
<a href="https://www.khronos.org/registry/OpenGL/extensions/OVR/OVR_multiview2.txt">https://www.khronos.org/registry/OpenGL/extensions/OVR/OVR_multiview2.txt</a><br>
<a href="https://community.arm.com/developer/tools-software/graphics/b/blog/posts/optimizing-virtual-reality-understanding-multiview">https://community.arm.com/developer/tools-software/graphics/b/blog/posts/optimizing-virtual-reality-understanding-multiview</a><br>
<a href="https://arm-software.github.io/opengl-es-sdk-for-android/multiview.html">https://arm-software.github.io/opengl-es-sdk-for-android/multiview.html</a><br>
<a href="https://github.com/KhronosGroup/WebGL/issues/2912">https://github.com/KhronosGroup/WebGL/issues/2912</a><br>
<a href="https://developer.oculus.com/documentation/oculus-browser/latest/concepts/browser-multiview/">https://developer.oculus.com/documentation/oculus-browser/latest/concepts/browser-multiview/</a></p>
<p><em>* Header image by Nvidia VRWorks</em></p>
]]></content:encoded></item><item><title><![CDATA[Firefox Reality 1.4]]></title><description><![CDATA[<p><strong>Firefox Reality 1.4 is now available for users in the Viveport and Oculus stores.</strong></p>
<p>With this release, we’re excited to announce that users can enjoy browsing in multiple windows side-by-side. Each window can be set to the size and position of your choice, for a super customizable experience.</p>]]></description><link>https://blog.mozvr.com/firefox-reality-1-4/</link><guid isPermaLink="false">5d5d9c1645b46a00386921f3</guid><category><![CDATA[vr]]></category><category><![CDATA[Firefox Reality]]></category><category><![CDATA[Updates]]></category><dc:creator><![CDATA[Janice Von Itter]]></dc:creator><pubDate>Wed, 11 Sep 2019 22:45:08 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/09/O_Hero.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/09/O_Hero.png" alt="Firefox Reality 1.4"><p><strong>Firefox Reality 1.4 is now available for users in the Viveport and Oculus stores.</strong></p>
<p>With this release, we’re excited to announce that users can enjoy browsing in multiple windows side-by-side. Each window can be set to the size and position of your choice, for a super customizable experience.</p>
<p><img src="https://blog.mozvr.com/content/images/2019/09/multiwindowscreenshot.jpg" alt="Firefox Reality 1.4"></p>
<p>And, by popular demand, we’ve enabled local browsing history, so you can get back to sites you've visited before without typing. Sites in your history will also appear as you type in the search bar, so you can complete the address quickly and easily. You can clear your history or turn it off anytime from within Settings.</p>
<p>The Content Feed also has a new and improved menu of hand-curated “Best of WebVR” content for you to explore. You can look forward to monthly updates featuring a selection of new content across different categories including  <em>Animation</em>, <em>Extreme</em> (sports/adrenaline/adventure), <em>Music</em>, <em>Art &amp; Experimental</em> and our personal favorite way to wind down a day, <em>360 Chill</em>.</p>
<p><strong>Additional highlights</strong></p>
<ul>
<li>Movable keyboard, so you can place it where it’s most comfortable to type.</li>
<li>Tooltips on buttons and actions throughout the app.</li>
<li>Updated look and feel for the Bookmarks and History views so you can see and interact better at all window sizes.</li>
<li>An easy way to request the desktop version of a site that doesn’t display well in VR, right from the search bar.</li>
<li>Updated and reorganized settings to be easier to find and understand.</li>
<li>Added the ability to set a preferred website language order.</li>
</ul>
<p>Full release notes can be found in our <a href="https://github.com/MozillaReality/FirefoxReality">GitHub repo here</a>.</p>
<p>Stay tuned as we keep improving Firefox Reality! We’re currently working on integrating your <a href="https://blog.mozilla.org/firefox/join-firefox/">Firefox Account</a> so you’ll be able to easily send tabs to and from VR from other devices. New languages and copy/paste are also coming soon, in addition to continued improvements in performance and stability.</p>
<p>Firefox Reality is available right now. Go and get it!<br>
<strong><a href="https://www.oculus.com/experiences/go/2208418715853974/">Download for Oculus Go</a></strong><br>
<strong><a href="https://www.oculus.com/experiences/quest/2180252408763702">Download for Oculus Quest</a></strong><br>
<strong>Download for Viveport</strong> (Search for <code>Firefox Reality</code> in Viveport store)</p>
]]></content:encoded></item><item><title><![CDATA[WebXR emulator extension]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>We are happy to announce the release of our WebXR emulator browser extension which helps WebXR content creation.</p>
<p>We understand that developing and debugging <a href="https://www.w3.org/TR/webxr/">WebXR</a> experiences is hard for many reasons:</p>
<ul>
<li>You must own a physical XR device</li>
<li>Lack of support of XR devices on some platforms, as macOS</li>
<li>Putting</li></ul>]]></description><link>https://blog.mozvr.com/webxr-emulator-extension/</link><guid isPermaLink="false">5d6957054e116e0038e88dcd</guid><category><![CDATA[WebXR]]></category><category><![CDATA[vr]]></category><category><![CDATA[extension]]></category><dc:creator><![CDATA[Takahiro Aoyagi]]></dc:creator><pubDate>Tue, 10 Sep 2019 17:09:05 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/12/big_screenshot.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://blog.mozvr.com/content/images/2019/12/big_screenshot.png" alt="WebXR emulator extension"><p>We are happy to announce the release of our WebXR emulator browser extension which helps WebXR content creation.</p>
<p>We understand that developing and debugging <a href="https://www.w3.org/TR/webxr/">WebXR</a> experiences is hard for many reasons:</p>
<ul>
<li>You must own a physical XR device</li>
<li>Lack of support of XR devices on some platforms, as macOS</li>
<li>Putting on and taking off the headset all the time is an uncomfortable task</li>
<li>In order to make your app responsive across form factors, you must own tons of devices: mobile, tethered, 3dof, 6dof, and so on</li>
</ul>
<p>With this extension, we aim to soften most of these issues.</p>
<p>WebXR emulator extension emulates XR devices so that you can directly enter immersive(VR) mode from your <strong>desktop</strong> browser and test your WebXR application without the need of any XR devices. It emulates multiple XR devices, so you can select which one you want to test.</p>
<p>The extension is built on top of the <a href="https://developer.mozilla.org/docs/Mozilla/Add-ons/WebExtensions">WebExtensions API</a>, so it works on Firefox, Chrome, and other browsers supporting the API.</p>
<p><img src="https://blog.mozvr.com/content/images/2019/12/screenshot-1.gif" alt="WebXR emulator extension"></p>
<iframe width="694" height="390" src="https://www.youtube.com/embed/uneEvM40dN8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="howcaniuseit">How can I use it?</h2>
<ol>
<li>Install the extension from the extension stores (<a href="https://addons.mozilla.org/firefox/addon/webxr-api-emulator">Firefox</a>, <a href="https://chrome.google.com/webstore/detail/webxr-api-emulator/mjddjgeghkdijejnciaefnkjmkafnnje">Chrome</a>)</li>
<li>Launch a WebXR application, for example the <a href="https://threejs.org/examples/?q=webxr#webxr_vr_ballshooter">Three.js examples</a>. You will notice that the application detects that you have a VR device (emulated) and it will let you enter the immersive (VR) mode.</li>
<li>Open the “WebXR” tab in the browser’s developer tool (<a href="https://developer.mozilla.org/docs/Tools">Firefox</a>, <a href="https://developers.google.com/web/tools/chrome-devtools/">Chrome</a>) to control the emulated device. You can move the headset and controllers and trigger the controller buttons. You will see their transforms reflected in the WebXR application.<br>
<img src="https://blog.mozvr.com/content/images/2019/12/tab.png" alt="WebXR emulator extension"></li>
</ol>
<h2 id="whatsnext">What’s next?</h2>
<p>The development of this extension is still at an early stage. We have many awesome features planned, including:</p>
<ul>
<li>Recording and replaying of actions and movements of your XR devices so you don’t have to replicate them every time you want to test your app and can share them with others.</li>
<li>Incorporate new XR devices</li>
<li>Control the headset and controllers using a standard gamepad like the Xbox or PS4 controllers or use your mobile as 3dof device</li>
<li>Something else?</li>
</ul>
<p>We would love your feedback! What new features do you want next? Any problems with the extension on your WebXR application? Please join us on <a href="https://github.com/MozillaReality/WebXR-emulator-extension">GitHub</a> to discuss them.</p>
<p>Lastly, we would like to give a shout out to the <a href="https://chrome.google.com/webstore/detail/webvr-api-emulation/gbdnpaebafagioggnhkacnaaahpiefil?hl=en">WebVR API emulation Extension</a> by <a href="https://twitter.com/thespite">Jaume Sanchez</a> as it was a true inspiration for us when building this one.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Semantic Placement in Augmented Reality using MrEd]]></title><description><![CDATA[In this article we’re going to take a brief look at how we may want to think about placement of objects in Augmented Reality.
]]></description><link>https://blog.mozvr.com/semantic-placement-in-ar/</link><guid isPermaLink="false">5d6ff54c8b6ebd003834f410</guid><category><![CDATA[WebXR]]></category><category><![CDATA[semantic placement]]></category><category><![CDATA[mozilla]]></category><dc:creator><![CDATA[Anselm Hook]]></dc:creator><pubDate>Thu, 05 Sep 2019 18:57:00 GMT</pubDate><media:content url="https://blog.mozvr.com/content/images/2019/09/Screen-Shot-2019-09-04-at-12.44.34-PM.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.mozvr.com/content/images/2019/09/Screen-Shot-2019-09-04-at-12.44.34-PM.png" alt="Semantic Placement in Augmented Reality using MrEd"><p>In this article we’re going to take a brief look at how we may want to think about placement of objects in Augmented Reality. We're going to use <a href="https://blog.mozvr.com/mred-an-experiment-in-mixed-reality-editing/">our recently released lightweight AR editing tool MrEd</a> to make this easy to demonstrate.</p>
<iframe src="https://giphy.com/embed/fxZrMskd67Bhgt30tQ" width="480" height="214" frameborder="0" class="giphy-embed" allowfullscreen></iframe><p><a href="https://giphy.com/gifs/duck-webxr-fxZrMskd67Bhgt30tQ"></a></p>
<p>Designers often express ideas in a domain appropriate language. For example a designer may say “place that chair on the floor” or “hang that photo at eye level on the wall”.</p>
<p>However when we finalize a virtual scene in 3d we often keep only the literal or absolute XYZ position of elements and throw out the original intent - the deeper reason why an object ended up in a certain position.</p>
<p>It turns out that it’s worth keeping the intention - so that when AR scenes are re-created for new participants or in new physical locations that the scenes still “work” - that they still are satisfying experiences - even if some aspects change.</p>
<p>In a sense this recognizes the Japanese term 'Wabi-Sabi'; that aesthetic placement is always imperfect and contends between fickle forces. Describing placement in terms of semantic intent is also similar to responsive design on the web or the idea of design patterns as described by Christopher Alexander.</p>
<p>Let’s look at two simple examples of semantic placement in practice.</p>
<p><strong>1. Relative to the Ground</strong></p>
<p>When you’re placing objects in augmented reality you often want to specify that those objects should be relationally placed in a position relative to other objects. A typical, in fact ubiquitous, example of placement is that often you want an object to be positioned relative to “the ground”.</p>
<p>Sometimes the designer's intent is to select the highest relative surface underneath the object in question (such as placing a lamp on a table) or at other times to select the lowest relative surface underneath an object (such as say placing a kitten on the floor under a table). Often, as well, we may want to express a placement in the air - such as say a mailbox, or a bird.</p>
<p>In this very small example I’ve attached a ground detection script to a duck, and then sprinkled a few other passive objects around the scene. As the ground is detected the duck will pop down from a default position to be offset relative to the ground (although still in the air). See the GIF above for an example of the effect.</p>
<p>To try this scene out yourself you will need <a href="https://apps.apple.com/us/app/webxr-viewer/id1295998056">WebXR for iOS</a> which is a preview of emerging WebXR standards using iOS ARKit to expose augmented reality features in a browser environment. This is the url for the scene above in play mode (on a WebXR capable device):</p>
<p><a href="https://painted-traffic.glitch.me/.mred/build/?mode=play&amp;doc=doc_103575453">https://painted-traffic.glitch.me/.mred/build/?mode=play&amp;doc=doc_103575453</a></p>
<p>Here is what it should look like in edit mode:</p>
<p><img src="https://blog.mozvr.com/content/images/2019/09/Screen-Shot-2019-09-05-at-11.44.10-AM.png" alt="Semantic Placement in Augmented Reality using MrEd"></p>
<p>You can also clone the glitch and edit the scene yourself (you’ll want to remember to set a password in the .env file and then login from inside MrEd). See:</p>
<p><a href="https://glitch.com/edit/#!/painted-traffic">https://glitch.com/edit/#!/painted-traffic</a></p>
<p>Here’s my script itself:</p>
<pre><code>/// #title grounded
/// #description Stick to Floor/Ground - dynamically and constantly searching for low areas nearby
({
    start: function(evt) {
        this.sgp.startWorldInfo()
    },
    tick: function(e) {
        let floor = this.sgp.getFloorNear({point:e.target.position})
        if(floor) {
            e.target.position.y = floor.y
        }
    }
})
</code></pre>
<p>This is relying on code baked into MrEd (specifically inside of <a href="https://github.com/MozillaReality/mred/blob/master/src/vr/XRWorldInfo.js">findFloorNear() in XRWorldInfo.js</a> if you really want to get detailed).</p>
<p>In the above example I begin by calling startWorldInfo() to start painting the ground planes (so that I can see them since it’s nice to have visual feedback). And, every tick, I call a floor finder subroutine which simply returns the best guess as to the floor in that area. The floor finder logic in this case is pre-defined but one could easily imagine other kinds of floor finding strategies that were more flexible.</p>
<p><strong>2. Follow the player</strong></p>
<p>Another common designer intent is to make sure that some content is always visible to the player. As designers in virtual or augmented reality it can be more challenging to direct a users attention to virtual objects. These are 3d immersive worlds, the player can be looking in any direction. Some kind of mechanic is needed to help make sure that the player sees what they need to see.</p>
<p>One common simple solution is to build an object that stays in front of the user. This can be itself a combination of multiple simpler behaviors. An object can be ordered to seek a position in front of the user, be at a certain height, and ideally billboarded so that any signage or message is always legible.</p>
<p>In this example a sign is decorated with two separate scripts, one to keep the sign in front of the player, and another to billboard the sign to face the player.</p>
<p><a href="https://painted-traffic.glitch.me/.mred/build/?mode=edit&amp;doc=doc_875751741&amp;doctype=vr">https://painted-traffic.glitch.me/.mred/build/?mode=edit&amp;doc=doc_875751741&amp;doctype=vr</a></p>
<iframe src="https://giphy.com/embed/LSFqbRiXjcoJc67bsu" width="480" height="204" frameborder="0" class="giphy-embed" allowfullscreen></iframe><p><a href="https://giphy.com/gifs/duck-webxr-semantic-LSFqbRiXjcoJc67bsu"></a></p>
<p><strong>Closing thoughts</strong></p>
<p>We’ve only scratched the surface of the kinds of intent could be expressed or combined together. If you want to dive deeper there is a longer list in a separate article <a href="https://arvrjourney.com/laundry-list-of-ux-patterns-in-vr-ar-24dae1e56c0a">Laundry List of UX Patterns</a>). I also invite you to help extend the industry; think both about what high level intentions you mean when you place objects and also how you'd communicate those intentions.</p>
<p>The key insight here is that preserving semantic intent means thinking of objects as intelligent, able to respond to simple high level goals. Virtual objects are more than just statues or art at a fixed position, but can be entities that can do your bidding, and follow high level rules.</p>
<p>Ultimately future 3d tools will almost certainly provide these kinds of services - much in the way CSS provides layout directives. We should also expect to see conventions emerge as more designers begin to work in this space. As a call to action, it's worth it to notice the high level intentions that you want, and to get the developers of the tools that you use to start to incorporate those intentions as primitives.</p>
]]></content:encoded></item></channel></rss>